{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "来自*GPyTorch*官方文档[Hyperparameters in GPyTorch](https://docs.gpytorch.ai/en/latest/examples/00_Basic_Usage/Hyperparameters.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先定义一个GP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "train_x = torch.linspace(0, 1, 100)\n",
    "train_y = torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2\n",
    "\n",
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "+ 查看model中的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExactGPModel(\n",
       "  (likelihood): GaussianLikelihood(\n",
       "    (noise_covar): HomoskedasticNoise(\n",
       "      (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "    )\n",
       "  )\n",
       "  (mean_module): ConstantMean()\n",
       "  (covar_module): ScaleKernel(\n",
       "    (base_kernel): RBFKernel(\n",
       "      (raw_lengthscale_constraint): Positive()\n",
       "    )\n",
       "    (raw_outputscale_constraint): Positive()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ExactGPModel in module __main__ object:\n",
      "\n",
      "class ExactGPModel(gpytorch.models.exact_gp.ExactGP)\n",
      " |  ExactGPModel(train_x, train_y, likelihood)\n",
      " |  \n",
      " |  The base class for any Gaussian process latent function to be used in conjunction\n",
      " |  with exact inference.\n",
      " |  \n",
      " |  :param torch.Tensor train_inputs: (size n x d) The training features :math:`\\mathbf X`.\n",
      " |  :param torch.Tensor train_targets: (size n) The training targets :math:`\\mathbf y`.\n",
      " |  :param ~gpytorch.likelihoods.GaussianLikelihood likelihood: The Gaussian likelihood that defines\n",
      " |      the observational distribution. Since we're using exact inference, the likelihood must be Gaussian.\n",
      " |  \n",
      " |  The :meth:`forward` function should describe how to compute the prior latent distribution\n",
      " |  on a given input. Typically, this will involve a mean and kernel function.\n",
      " |  The result must be a :obj:`~gpytorch.distributions.MultivariateNormal`.\n",
      " |  \n",
      " |  Calling this model will return the posterior of the latent Gaussian process when conditioned\n",
      " |  on the training data. The output will be a :obj:`~gpytorch.distributions.MultivariateNormal`.\n",
      " |  \n",
      " |  Example:\n",
      " |      >>> class MyGP(gpytorch.models.ExactGP):\n",
      " |      >>>     def __init__(self, train_x, train_y, likelihood):\n",
      " |      >>>         super().__init__(train_x, train_y, likelihood)\n",
      " |      >>>         self.mean_module = gpytorch.means.ZeroMean()\n",
      " |      >>>         self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
      " |      >>>\n",
      " |      >>>     def forward(self, x):\n",
      " |      >>>         mean = self.mean_module(x)\n",
      " |      >>>         covar = self.covar_module(x)\n",
      " |      >>>         return gpytorch.distributions.MultivariateNormal(mean, covar)\n",
      " |      >>>\n",
      " |      >>> # train_x = ...; train_y = ...\n",
      " |      >>> likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
      " |      >>> model = MyGP(train_x, train_y, likelihood)\n",
      " |      >>>\n",
      " |      >>> # test_x = ...;\n",
      " |      >>> model(test_x)  # Returns the GP latent function at test_x\n",
      " |      >>> likelihood(model(test_x))  # Returns the (approximate) predictive posterior distribution at test_x\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ExactGPModel\n",
      " |      gpytorch.models.exact_gp.ExactGP\n",
      " |      gpytorch.models.gp.GP\n",
      " |      gpytorch.module.Module\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, train_x, train_y, likelihood)\n",
      " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  forward(self, x)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gpytorch.models.exact_gp.ExactGP:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  get_fantasy_model(self, inputs, targets, **kwargs)\n",
      " |      Returns a new GP model that incorporates the specified inputs and targets as new training data.\n",
      " |      \n",
      " |      Using this method is more efficient than updating with `set_train_data` when the number of inputs is relatively\n",
      " |      small, because any computed test-time caches will be updated in linear time rather than computed from scratch.\n",
      " |      \n",
      " |      .. note::\n",
      " |          If `targets` is a batch (e.g. `b x m`), then the GP returned from this method will be a batch mode GP.\n",
      " |          If `inputs` is of the same (or lesser) dimension as `targets`, then it is assumed that the fantasy points\n",
      " |          are the same for each target batch.\n",
      " |      \n",
      " |      :param torch.Tensor inputs: (`b1 x ... x bk x m x d` or `f x b1 x ... x bk x m x d`) Locations of fantasy\n",
      " |          observations.\n",
      " |      :param torch.Tensor targets: (`b1 x ... x bk x m` or `f x b1 x ... x bk x m`) Labels of fantasy observations.\n",
      " |      :return: An `ExactGP` model with `n + m` training examples, where the `m` fantasy examples have been added\n",
      " |          and all test-time caches have been updated.\n",
      " |      :rtype: ~gpytorch.models.ExactGP\n",
      " |  \n",
      " |  local_load_samples(self, samples_dict, memo, prefix)\n",
      " |      Replace the model's learned hyperparameters with samples from a posterior distribution.\n",
      " |  \n",
      " |  set_train_data(self, inputs=None, targets=None, strict=True)\n",
      " |      Set training data (does not re-fit model hyper-parameters).\n",
      " |      \n",
      " |      :param torch.Tensor inputs: The new training inputs.\n",
      " |      :param torch.Tensor targets: The new training targets.\n",
      " |      :param bool strict: (default True) If `True`, the new inputs and\n",
      " |          targets must have the same shape, dtype, and device\n",
      " |          as the current inputs and targets. Otherwise, any shape/dtype/device are allowed.\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gpytorch.models.exact_gp.ExactGP:\n",
      " |  \n",
      " |  train_targets\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gpytorch.module.Module:\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  added_loss_terms(self)\n",
      " |  \n",
      " |  constraint_for_parameter_name(self, param_name)\n",
      " |  \n",
      " |  constraints(self)\n",
      " |  \n",
      " |  hyperparameters(self)\n",
      " |  \n",
      " |  initialize(self, **kwargs)\n",
      " |      Set a value for a parameter\n",
      " |      \n",
      " |      kwargs: (param_name, value) - parameter to initialize.\n",
      " |      Can also initialize recursively by passing in the full name of a\n",
      " |      parameter. For example if model has attribute model.likelihood,\n",
      " |      we can initialize the noise with either\n",
      " |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      " |      or\n",
      " |      `model.likelihood.initialize(noise=0.1)`.\n",
      " |      The former method would allow users to more easily store the\n",
      " |      initialization values as one object.\n",
      " |      \n",
      " |      Value can take the form of a tensor, a float, or an int\n",
      " |  \n",
      " |  named_added_loss_terms(self)\n",
      " |      Returns an iterator over module variational strategies, yielding both\n",
      " |      the name of the variational strategy as well as the strategy itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, VariationalStrategy): Tuple containing the name of the\n",
      " |              strategy and the strategy\n",
      " |  \n",
      " |  named_constraints(self, memo=None, prefix='')\n",
      " |  \n",
      " |  named_hyperparameters(self)\n",
      " |  \n",
      " |  named_parameters_and_constraints(self)\n",
      " |  \n",
      " |  named_priors(self, memo=None, prefix='')\n",
      " |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      " |      the prior, the associated parameter names, and the transformation callable.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      " |              - the name of the prior\n",
      " |              - the prior\n",
      " |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      " |              - the prior's transform to be called on the parameters\n",
      " |  \n",
      " |  named_variational_parameters(self)\n",
      " |  \n",
      " |  pyro_load_from_samples(self, samples_dict)\n",
      " |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      " |      is typically produced by a Pyro sampling mechanism.\n",
      " |      \n",
      " |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      " |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      " |      the prior to properly set the unconstrained parameter.\n",
      " |      \n",
      " |      Args:\n",
      " |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      " |  \n",
      " |  pyro_sample_from_prior(self)\n",
      " |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      " |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      " |      \n",
      " |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      " |      parameters of the model that have GPyTorch priors registered to them.\n",
      " |  \n",
      " |  register_added_loss_term(self, name)\n",
      " |  \n",
      " |  register_constraint(self, param_name, constraint, replace=True)\n",
      " |  \n",
      " |  register_parameter(self, name, parameter, prior=None)\n",
      " |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          :attr:`name` (str):\n",
      " |              The name of the parameter\n",
      " |          :attr:`parameter` (torch.nn.Parameter):\n",
      " |              The parameter\n",
      " |  \n",
      " |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      " |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          :attr:`name` (str):\n",
      " |              The name of the prior\n",
      " |          :attr:`prior` (Prior):\n",
      " |              The prior to be registered`\n",
      " |          :attr:`param_or_closure` (string or callable):\n",
      " |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      " |              one or more parameters):\n",
      " |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      " |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      " |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda: torch.log(self.foo_param))`\n",
      " |              function of multiple parameters:\n",
      " |              `.register_prior(\"foo2_prior\", foo2_prior, lambda: f(self.param1, self.param2)))`\n",
      " |          :attr:`setting_closure` (callable, optional):\n",
      " |              A function taking in a tensor in (transformed) parameter space and initializing the\n",
      " |              internal parameter representation to the proper value by applying the inverse transform.\n",
      " |              Enables setting parametres directly in the transformed space, as well as sampling\n",
      " |              parameter values from priors (see `sample_from_prior`)\n",
      " |  \n",
      " |  sample_from_prior(self, prior_name)\n",
      " |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      " |  \n",
      " |  update_added_loss_term(self, name, added_loss_term)\n",
      " |  \n",
      " |  variational_parameters(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.data.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  buffers(self, recurse=True)\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf.data), buf.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self, device=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self)\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should reimplement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all floating point parameters and buffers to float datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict, strict=True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix='', recurse=True)\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>    if name in ['running_var']:\n",
      " |          >>>        print(buf.size())\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix='', recurse=True)\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse=True)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |      \n",
      " |      .. warning ::\n",
      " |      \n",
      " |          The current implementation will not have the presented behavior\n",
      " |          for complex :class:`Module` that perform many operations.\n",
      " |          In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only\n",
      " |          contain the gradients for a subset of the inputs and outputs.\n",
      " |          For such :class:`Module`, you should use :func:`torch.Tensor.register_hook`\n",
      " |          directly on a specific input or output to get the required gradients.\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None or modified output\n",
      " |      \n",
      " |      The hook can modify the output. It can modify the input inplace but\n",
      " |      it will not have effect on forward since this is called after\n",
      " |      :func:`forward` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None or modified input\n",
      " |      \n",
      " |      The hook can modify the input. User can either return a tuple or a\n",
      " |      single modified value in the hook. We will wrap the value into a tuple\n",
      " |      if a single value is returned(unless that value is already a tuple).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  requires_grad_(self, requires_grad=True)\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point desired :attr:`dtype` s. In addition, this method will\n",
      " |      only cast the floating point parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point type of\n",
      " |              the floating point parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的文档中，可以查看到model的继承关系：    \n",
    "Method resolution order:\n",
    " |      ExactGPModel   \n",
    " |      gpytorch.models.exact_gp.ExactGP    \n",
    " |      gpytorch.models.gp.GP   \n",
    " |      gpytorch.module.Module   \n",
    " |      torch.nn.modules.module.Module   \n",
    " |      builtins.object   \n",
    " \n",
    "*有关parameter的方法继承自：*  \n",
    "Methods inherited from torch.nn.modules.module.Module\n",
    "\n",
    "\n",
    "#用法为：  \n",
    " |  **named_parameters(self, prefix='', recurse=True)**    \n",
    " |      Returns an iterator over module parameters, yielding both the   \n",
    " |      name of the parameter as well as the parameter itself.   \n",
    " |      \n",
    " |      Args:   \n",
    " |          prefix (str): prefix to prepend to all parameter names.   \n",
    " |          recurse (bool): if True, then yields parameters of this module   \n",
    " |              and all submodules. Otherwise, yields only parameters that   \n",
    " |              are direct members of this module.   \n",
    " |      \n",
    " |      Yields:   \n",
    " |          (string, Parameter): Tuple containing the name and parameter   \n",
    " |      \n",
    " |      Example::    \n",
    " |      \n",
    " |          >>> for name, param in self.named_parameters():   \n",
    " |          >>>    if name in ['bias']:  \n",
    " |          >>>        print(param.size())  \n",
    " |  \n",
    " |  **parameters(self, recurse=True)**   \n",
    " |      Returns an iterator over module parameters.  \n",
    " |          \n",
    " |      This is typically passed to an optimizer.   \n",
    " |         \n",
    " |      Args:   \n",
    " |          recurse (bool): if True, then yields parameters of this module   \n",
    " |              and all submodules. Otherwise, yields only parameters that  \n",
    " |              are direct members of this module.   \n",
    " |      \n",
    " |      Yields:  \n",
    " |          Parameter: module parameter   \n",
    " |      \n",
    " |      Example::   \n",
    " |        \n",
    " |          >>> for param in model.parameters():  \n",
    " |          >>>     print(type(param.data), param.size())  \n",
    " |          <class 'torch.FloatTensor'> (20L,)   \n",
    " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExactGPModel(\n",
       "  (likelihood): GaussianLikelihood(\n",
       "    (noise_covar): HomoskedasticNoise(\n",
       "      (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "    )\n",
       "  )\n",
       "  (mean_module): ConstantMean()\n",
       "  (covar_module): ScaleKernel(\n",
       "    (base_kernel): RBFKernel(\n",
       "      (raw_lengthscale_constraint): Positive()\n",
       "    )\n",
       "    (raw_outputscale_constraint): Positive()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x0000028402222F48>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor(0., requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[0.]], requires_grad=True)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('likelihood.noise_covar.raw_noise', Parameter containing:\n",
      "tensor([0.], requires_grad=True)), ('mean_module.constant', Parameter containing:\n",
      "tensor([0.], requires_grad=True)), ('covar_module.raw_outputscale', Parameter containing:\n",
      "tensor(0., requires_grad=True)), ('covar_module.base_kernel.raw_lengthscale', Parameter containing:\n",
      "tensor([[0.]], requires_grad=True))]\n",
      "Parameter name: likelihood.noise_covar.raw_noise           value = 0.0\n",
      "Parameter name: mean_module.constant                       value = 0.0\n",
      "Parameter name: covar_module.raw_outputscale               value = 0.0\n",
      "Parameter name: covar_module.base_kernel.raw_lengthscale   value = 0.0\n"
     ]
    }
   ],
   "source": [
    "print(list(model.named_parameters()))\n",
    "for param_name, param in model.named_parameters():\n",
    "    print(f'Parameter name: {param_name:42} value = {param.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "+ **Raw vs Actual Parameters**   \n",
    "actual parameter一般都需要有限制（constraints），比如，  \n",
    "+ scale kernel中的output scale需要为正数   \n",
    "+ RBF kernel中的lengthscale 也需要为正数\n",
    "+ likelihood中的加性噪声方差需要大于一定的值 等   \n",
    "\n",
    "为了加入这些限制，对actual parameter进行变形，引入raw parameter，   \n",
    "例如，对于要大于零的参数$\\theta$,引入raw参数$\\lambda\\$:  \n",
    "$$\\theta=e^{\\lambda}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "+ **查看raw参数值以及参数限制**    \n",
    "调用类的属性，或者使用.named_paramters(), .named_constraints()方法；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaleKernel(\n",
      "  (base_kernel): RBFKernel(\n",
      "    (raw_lengthscale_constraint): Positive()\n",
      "  )\n",
      "  (raw_outputscale_constraint): Positive()\n",
      ")\n",
      "[('raw_outputscale', Parameter containing:\n",
      "tensor(0., requires_grad=True)), ('base_kernel.raw_lengthscale', Parameter containing:\n",
      "tensor([[0.]], requires_grad=True))]\n",
      "[Positive(), Positive()]\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "print(model.covar_module)\n",
    "print(list(model.covar_module.named_parameters()))\n",
    "print(list(model.covar_module.constraints()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_outputscale,  Parameter containing:\n",
      "tensor(0., requires_grad=True)\n",
      "raw_outputscale constraint  Positive()\n"
     ]
    }
   ],
   "source": [
    "print('raw_outputscale, ', model.covar_module.raw_outputscale)\n",
    "print('raw_outputscale constraint ', model.covar_module.raw_outputscale_constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_lengthscale,  Parameter containing:\n",
      "tensor([[0.]], requires_grad=True)\n",
      "raw_lengthscale constraint  Positive()\n"
     ]
    }
   ],
   "source": [
    "print('raw_lengthscale, ', model.covar_module.base_kernel.raw_lengthscale)\n",
    "print('raw_lengthscale constraint ', model.covar_module.base_kernel.raw_lengthscale_constraint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **raw param 与 actual param间的互相转换**   \n",
    "利用constraint.transform(), .inverse.transform()方法；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed outputscale tensor(0.6931, grad_fn=<SoftplusBackward>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "Transform a bunch of negative tensors:  tensor([0.3133, 0.1269, 0.0486])\n"
     ]
    }
   ],
   "source": [
    "raw_outputscale = model.covar_module.raw_outputscale\n",
    "constraint = model.covar_module.raw_outputscale_constraint\n",
    "\n",
    "trans_param = constraint.transform(raw_outputscale)\n",
    "print('Transformed outputscale', trans_param)\n",
    "\n",
    "print(constraint.inverse_transform(trans_param))\n",
    "\n",
    "print('Transform a bunch of negative tensors: ', constraint.transform(torch.tensor([-1., -2., -3.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed lengthscale tensor([[0.6931]], grad_fn=<SoftplusBackward>)\n",
      "tensor([[0.]], grad_fn=<AddBackward0>)\n",
      "Transform a bunch of negative tensors:  tensor([0.3133, 0.1269, 0.0486])\n"
     ]
    }
   ],
   "source": [
    "raw_lengthscale = model.covar_module.base_kernel.raw_lengthscale\n",
    "constraint = model.covar_module.base_kernel.raw_lengthscale_constraint\n",
    "\n",
    "trans_param = constraint.transform(raw_lengthscale)\n",
    "print('Transformed lengthscale', trans_param)\n",
    "\n",
    "print(constraint.inverse_transform(trans_param))\n",
    "\n",
    "print('Transform a bunch of negative tensors: ', constraint.transform(torch.tensor([-1., -2., -3.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 这里的softplus函数为：  \n",
    "$$ f(x) = \\ln (1+e^x)$$   \n",
    "该函数恒大于0， 上面的x为raw， 计算出的函数值为真实的param  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2840228d988>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.linspace(-5,5, 200)\n",
    "y = torch.log(1 + torch.exp(x))\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 其他修改超参数的办法"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_gpu] *",
   "language": "python",
   "name": "conda-env-pytorch_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
